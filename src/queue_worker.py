# src/worker/queue_worker.py
import asyncio
import json
import logging
import os
import socket
import subprocess
import sys
from typing import Dict, Optional

from aiogram.types import Update

from shared.database import MasterDatabase, get_master_dsn
from shared.cleanup_tasks import QueueCleanupService
from worker.main import GraceHubWorker

logger = logging.getLogger("queue_worker")


def _get_int_env(name: str, default: int) -> int:
    v = os.getenv(name)
    if v is None or v.strip() == "":
        return default
    try:
        return int(v)
    except ValueError:
        logger.warning("Invalid int env %s=%r, using default=%s", name, v, default)
        return default


def _get_float_env(name: str, default: float) -> float:
    v = os.getenv(name)
    if v is None or v.strip() == "":
        return default
    try:
        return float(v)
    except ValueError:
        logger.warning("Invalid float env %s=%r, using default=%s", name, v, default)
        return default


def _worker_id() -> str:
    # ÑƒÐ´Ð¾Ð±Ð½Ð¾ Ð²Ð¸Ð´ÐµÑ‚ÑŒ, ÐºÐ°ÐºÐ¾Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð·Ð°Ð»Ð¾Ñ‡Ð¸Ð» job
    return os.getenv("QUEUE_WORKER_ID") or f"{socket.gethostname()}:{os.getpid()}"


async def _get_or_create_worker(
    cache: Dict[str, GraceHubWorker],
    db: MasterDatabase,
    instance_id: str,
) -> Optional[GraceHubWorker]:
    """
    ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð¸Ð»Ð¸ ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ worker Ð´Ð»Ñ instance_id.
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ None, ÐµÑÐ»Ð¸ Ñ‚Ð¾ÐºÐµÐ½ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð¸Ð»Ð¸ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ð²Ð°Ð»Ð¸Ð»Ð°ÑÑŒ.
    """
    w = cache.get(instance_id)
    if w:
        return w

    token = await db.get_decrypted_token(instance_id)
    if not token:
        logger.warning(f"No token found for instance {instance_id}")
        return None

    # â— Ð’ÐÐ–ÐÐž: Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² GraceHubWorker(instance_id, db, token=...)
    w = GraceHubWorker(instance_id=instance_id, db=db, token=token)

    try:
        await w.initialize()
        logger.info(f"âœ… Worker {instance_id} initialized successfully (bot: {w.bot_username})")
    except Exception as e:
        logger.error(
            f"âŒ Failed to initialize worker {instance_id}: {type(e).__name__}: {e}",
            exc_info=True
        )
        return None

    cache[instance_id] = w
    return w


async def stuck_requeue_loop(
    db: MasterDatabase,
    *,
    stuck_seconds: int,
    interval_seconds: int,
) -> None:
    """
    ÐŸÐµÑ€ÐµÐºÐ¸Ð´Ñ‹Ð²Ð°ÐµÑ‚ Ð·Ð°Ð²Ð¸ÑÑˆÐ¸Ðµ jobs Ð¸Ð· status=processing Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð² retry.
    Ð­Ñ‚Ð¾ must-have, Ð¸Ð½Ð°Ñ‡Ðµ Ð¿Ð¾ÑÐ»Ðµ ÐºÑ€ÑÑˆÐ° Ð²Ð¾Ñ€ÐºÐµÑ€Ð° job Ð¼Ð¾Ð¶ÐµÑ‚ Ð¾ÑÑ‚Ð°Ñ‚ÑŒÑÑ processing Ð½Ð°Ð²ÑÐµÐ³Ð´Ð°.
    
    âš ï¸ DEPRECATED: Ð­Ñ‚Ð° Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸,
    Ð½Ð¾ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ñ€ÐµÐºÐ²ÐµÐ¹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ Ñ‡ÐµÑ€ÐµÐ· QueueCleanupService.
    """
    logger.warning(
        "âš ï¸ stuck_requeue_loop is DEPRECATED - requeue now handled by QueueCleanupService"
    )
    while True:
        try:
            n = await db.requeue_stuck_tg_updates(stuck_seconds=stuck_seconds)
            if n:
                logger.warning("Requeued stuck tg updates: %s", n)
        except Exception:
            logger.exception("stuck_requeue_loop failed")
        await asyncio.sleep(interval_seconds)


async def run_worker() -> None:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s pid=%(process)d %(name)s %(levelname)s - %(message)s",
    )

    dsn = get_master_dsn()
    db = MasterDatabase(dsn=dsn)
    await db.init()

    wid = _worker_id()
    logger.info("Queue worker started worker_id=%s", wid)

    # env-config
    idle_sleep = _get_float_env("QUEUE_WORKER_IDLE_SLEEP", 0.2)

    # ðŸ”¥ ÐÐžÐ’ÐÐ¯ Ð¡Ð˜Ð¡Ð¢Ð•ÐœÐ ÐžÐ§Ð˜Ð¡Ð¢ÐšÐ˜ Ñ‡ÐµÑ€ÐµÐ· QueueCleanupService
    cleanup_enabled = os.getenv("QUEUE_CLEANUP_ENABLED", "1").strip().lower() not in ("0", "false", "no")
    
    # Ð¡Ñ‚Ð°Ñ€Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ñ€ÐµÐºÐ²ÐµÑ (Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸, Ð½Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ)
    requeue_enabled = os.getenv("QUEUE_REQUEUE_ENABLED", "0").strip().lower() not in ("0", "false", "no")
    stuck_seconds = _get_int_env("QUEUE_REQUEUE_STUCK_SECONDS", 600)
    requeue_interval = _get_int_env("QUEUE_REQUEUE_INTERVAL_SECONDS", 30)

    fail_max_attempts = _get_int_env("QUEUE_FAIL_MAX_ATTEMPTS", 10)
    fail_retry_seconds = _get_int_env("QUEUE_FAIL_RETRY_SECONDS", 5)

    no_token_max_attempts = _get_int_env("QUEUE_NO_TOKEN_MAX_ATTEMPTS", 1)
    no_token_retry_seconds = _get_int_env("QUEUE_NO_TOKEN_RETRY_SECONDS", 0)

    cache: Dict[str, GraceHubWorker] = {}

    # ðŸ”¥ Ð—ÐÐŸÐ£Ð¡ÐšÐÐ•Ðœ ÐÐžÐ’Ð«Ð™ Ð¡Ð•Ð Ð’Ð˜Ð¡ ÐžÐ§Ð˜Ð¡Ð¢ÐšÐ˜ (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ)
    cleanup_service = None
    if cleanup_enabled:
        cleanup_service = QueueCleanupService(db)
        cleanup_service.start()
        logger.info("âœ… QueueCleanupService started (handles requeue + cleanup)")
    else:
        logger.warning("âš ï¸ QueueCleanupService disabled via QUEUE_CLEANUP_ENABLED=0")

    # Ð¡Ñ‚Ð°Ñ€Ñ‹Ð¹ Ñ€ÐµÐºÐ²ÐµÐ¹-Ð»ÑƒÐ¿ (deprecated, Ð½Ð¾ Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð´Ð»Ñ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸)
    if requeue_enabled and not cleanup_enabled:
        asyncio.create_task(
            stuck_requeue_loop(
                db,
                stuck_seconds=stuck_seconds,
                interval_seconds=requeue_interval,
            )
        )
        logger.info(
            "âš ï¸ Using deprecated stuck_requeue_loop: stuck_seconds=%s interval_seconds=%s",
            stuck_seconds,
            requeue_interval,
        )
        logger.warning("ðŸ’¡ Consider migrating to QUEUE_CLEANUP_ENABLED=1")
    elif requeue_enabled and cleanup_enabled:
        logger.info("â„¹ï¸ QUEUE_REQUEUE_ENABLED ignored (QueueCleanupService active)")

    try:
        while True:
            job = await db.pick_tg_update(worker_id=wid)
            if not job:
                await asyncio.sleep(idle_sleep)
                continue

            job_id = int(job["id"])
            instance_id = job["instance_id"]
            payload = job["payload"]

            try:
                worker = await _get_or_create_worker(cache, db, instance_id)
                if not worker:
                    await db.fail_tg_update(
                        job_id,
                        f"no token for instance_id={instance_id}",
                        max_attempts=no_token_max_attempts,
                        retry_seconds=no_token_retry_seconds,
                    )
                    continue

                if isinstance(payload, str):
                    payload = json.loads(payload)

                update = Update(**payload)
                await worker.process_update(update)

                await db.ack_tg_update(job_id)

            except Exception as e:
                await db.fail_tg_update(
                    job_id,
                    f"{type(e).__name__}: {e}",
                    max_attempts=fail_max_attempts,
                    retry_seconds=fail_retry_seconds,
                )
                logger.exception("Job failed id=%s instance_id=%s", job_id, instance_id)

    except KeyboardInterrupt:
        logger.info("ðŸ›‘ Received shutdown signal, stopping gracefully...")
    finally:
        # ðŸ”¥ GRACEFUL SHUTDOWN Ð´Ð»Ñ cleanup service
        if cleanup_service:
            logger.info("â³ Stopping QueueCleanupService...")
            await cleanup_service.stop()
        
        # Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÑƒÐ» Ð‘Ð”
        if db.pool:
            await db.pool.close()
            logger.info("âœ… Database pool closed")
        
        logger.info("ðŸ‘‹ Queue worker shutdown complete")


def run_supervisor() -> None:
    """
    Supervisor-Ñ€ÐµÐ¶Ð¸Ð¼: ÑÐ¿Ð°ÑƒÐ½Ð¸Ñ‚ N Ð²Ð¾Ñ€ÐºÐµÑ€Ð¾Ð² ÐºÐ°Ðº subprocess.
    ÐŸÐ¾Ð·Ð¶Ðµ ÑÑ‚Ð¾ Ð¿Ð¾Ñ‡Ñ‚Ð¸ 1-Ð²-1 Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑÑ Ð½Ð° docker compose --scale.
    """
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s pid=%(process)d %(name)s %(levelname)s - %(message)s",
    )

    replicas = _get_int_env("QUEUE_WORKER_REPLICAS", 1)
    replicas = max(1, replicas)

    logger.info("Supervisor starting replicas=%s", replicas)

    procs: list[subprocess.Popen] = []
    for i in range(replicas):
        env = os.environ.copy()
        env["QUEUE_WORKER_MODE"] = "worker"
        # Ð´ÐµÐ»Ð°ÐµÐ¼ worker_id Ð±Ð¾Ð»ÐµÐµ Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ñ‹Ð¼: hostname:pid:idx (pid Ð±ÑƒÐ´ÐµÑ‚ ÑƒÐ¶Ðµ Ð´Ð¾Ñ‡ÐµÑ€Ð½Ð¸Ð¹)
        env["QUEUE_WORKER_ID"] = env.get("QUEUE_WORKER_ID") or f"{socket.gethostname()}:replica-{i+1}"

        cmd = [sys.executable, __file__]
        p = subprocess.Popen(cmd, env=env)
        procs.append(p)
        logger.info("Spawned replica %s pid=%s", i + 1, p.pid)

    # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ "Ð¾Ð¶Ð¸Ð´Ð°Ñ‚ÐµÐ»ÑŒ": ÐµÑÐ»Ð¸ Ð»ÑŽÐ±Ð¾Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ ÑƒÐ¿Ð°Ð» â€” Ð²Ð°Ð»Ð¸Ð¼ supervisor (Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ‚Ñ‹ Ð·Ð°Ð¼ÐµÑ‚Ð¸Ð»)
    # ÐŸÑ€Ð¸ Ð¶ÐµÐ»Ð°Ð½Ð¸Ð¸ Ð¼Ð¾Ð¶Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾-Ñ€ÐµÑÑ‚Ð°Ñ€Ñ‚.
    try:
        while True:
            for p in procs:
                code = p.poll()
                if code is not None:
                    raise RuntimeError(f"Replica pid={p.pid} exited with code={code}")
            asyncio.run(asyncio.sleep(1.0))
    except KeyboardInterrupt:
        logger.info("Supervisor received KeyboardInterrupt, terminating replicas...")
    finally:
        for p in procs:
            try:
                p.terminate()
            except Exception:
                pass


def main() -> None:
    mode = os.getenv("QUEUE_WORKER_MODE", "worker").strip().lower()
    if mode == "supervisor":
        run_supervisor()
        return
    asyncio.run(run_worker())


if __name__ == "__main__":
    main()
